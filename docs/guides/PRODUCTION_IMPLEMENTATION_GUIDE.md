# Production Implementation Guide

## AI/ML Pipeline for Payer-Hub Integration

This guide provides practical implementation details, deployment strategies, and best practices for running the AI/ML unstructured data processing pipeline in production.

---

## ðŸ—ï¸ Architecture Deployment Patterns

### Pattern 1: Microservices Architecture (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API Gateway (FastAPI)                      â”‚
â”‚                    Port 8000 | Load Balanced                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚              â”‚              â”‚
  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”‚
  â”‚  OCR      â”‚      â”‚  NLP Extractor  â”‚   â”‚              â”‚
  â”‚  Service  â”‚      â”‚  Service        â”‚   â”‚              â”‚
  â”‚  (GPU)    â”‚      â”‚  (GPU)          â”‚   â”‚              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚              â”‚
                                            â”‚              â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                                  â”‚ FHIR Mapper      â”‚    â”‚
                                  â”‚ Service          â”‚    â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                                                          â”‚
                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                            â”‚ Anomaly Detection  â”‚
                                            â”‚ Service            â”‚
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Privacy Layer  â”‚                         â”‚   Event Middleware â”‚
           â”‚ Service        â”‚                         â”‚   (Kafka)          â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Docker Compose Setup

```yaml
# docker-compose.yml
version: '3.8'

services:
  # API Gateway
  api-gateway:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - REDIS_URL=redis://redis:6379
      - OCR_SERVICE_URL=http://ocr-service:8001
      - NLP_SERVICE_URL=http://nlp-service:8002
    depends_on:
      - kafka
      - redis
      - ocr-service
      - nlp-service
    volumes:
      - ./logs:/app/logs
    networks:
      - payerhub-network

  # OCR Processing Service
  ocr-service:
    build:
      context: .
      dockerfile: Dockerfile.ocr
    ports:
      - "8001:8001"
    environment:
      - MODEL_PATH=/app/models
      - USE_GPU=true
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - payerhub-network

  # NLP Extraction Service
  nlp-service:
    build:
      context: .
      dockerfile: Dockerfile.nlp
    ports:
      - "8002:8002"
    environment:
      - MODEL_PATH=/app/models
      - USE_BIOBERT=true
      - USE_CLINICAL_BERT=true
    volumes:
      - ./models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - payerhub-network

  # FHIR Mapper Service
  fhir-mapper:
    build:
      context: .
      dockerfile: Dockerfile.fhir
    ports:
      - "8003:8003"
    environment:
      - FHIR_SERVER_URL=http://fhir-server:8080/fhir
    networks:
      - payerhub-network

  # Anomaly Detection Service
  anomaly-detector:
    build:
      context: .
      dockerfile: Dockerfile.anomaly
    ports:
      - "8004:8004"
    environment:
      - MODEL_PATH=/app/models/anomaly_detector.pkl
    volumes:
      - ./models:/app/models
    networks:
      - payerhub-network

  # Privacy & Consent Service
  privacy-service:
    build:
      context: .
      dockerfile: Dockerfile.privacy
    ports:
      - "8005:8005"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/payerhub
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
    depends_on:
      - postgres
    networks:
      - payerhub-network

  # Kafka Event Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    networks:
      - payerhub-network

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - payerhub-network

  # Redis Cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
      - payerhub-network

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: payerhub
      POSTGRES_USER: user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - payerhub-network

  # Kafka UI (monitoring)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      - kafka
    networks:
      - payerhub-network

volumes:
  postgres-data:

networks:
  payerhub-network:
    driver: bridge
```

---

## ðŸ³ Dockerfile Examples

### API Gateway Dockerfile

```dockerfile
# Dockerfile.api
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY config/ ./config/

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "src.api_gateway.gateway:app", "--host", "0.0.0.0", "--port", "8000"]
```

### OCR Service Dockerfile (GPU-enabled)

```dockerfile
# Dockerfile.ocr
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

WORKDIR /app

# Install Python and system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    tesseract-ocr \
    poppler-utils \
    libgl1-mesa-glx \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
COPY requirements-ocr.txt .
RUN pip3 install --no-cache-dir -r requirements-ocr.txt

# Install PyTorch with CUDA support
RUN pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Copy code
COPY src/ai_pipeline/ocr_processor.py ./src/ai_pipeline/
COPY src/api/ocr_api.py ./src/api/

# Download models on build
RUN python3 -c "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification; \
    LayoutLMv3Processor.from_pretrained('microsoft/layoutlmv3-base'); \
    LayoutLMv3ForTokenClassification.from_pretrained('microsoft/layoutlmv3-base')"

EXPOSE 8001

CMD ["uvicorn", "src.api.ocr_api:app", "--host", "0.0.0.0", "--port", "8001"]
```

---

## ðŸ”§ FastAPI Service Implementation

### OCR Service API

```python
# src/api/ocr_api.py
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from src.ai_pipeline.ocr_processor import OCRProcessor
import tempfile
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="OCR Processing Service", version="1.0.0")

# Initialize OCR processor
config = {
    'use_layout_model': True,
    'tesseract_config': '--oem 3 --psm 6'
}
ocr_processor = OCRProcessor(config)

@app.post("/process")
async def process_document(file: UploadFile = File(...)):
    """
    Process uploaded document with OCR
    Returns extracted text and layout structure
    """
    try:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name
        
        # Process with OCR
        logger.info(f"Processing document: {file.filename}")
        result = ocr_processor.process_document(tmp_path)
        
        # Clean up
        os.unlink(tmp_path)
        
        return JSONResponse(content={
            'status': 'success',
            'filename': file.filename,
            'result': result
        })
    
    except Exception as e:
        logger.error(f"OCR processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {'status': 'healthy', 'service': 'ocr-processing'}

@app.get("/metrics")
async def metrics():
    """Expose metrics for monitoring"""
    return {
        'service': 'ocr-processing',
        'model': 'layoutlmv3-base',
        'gpu_available': torch.cuda.is_available(),
        'version': '1.0.0'
    }
```

### NLP Extraction Service API

```python
# src/api/nlp_api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from src.ai_pipeline.nlp_extractor import MedicalNLPExtractor
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="NLP Extraction Service", version="1.0.0")

# Initialize NLP extractor
config = {
    'use_biobert': True,
    'use_clinical_bert': True
}
nlp_extractor = MedicalNLPExtractor(config)

class ExtractionRequest(BaseModel):
    text: str
    layout_structure: dict = None

class ExtractionResponse(BaseModel):
    status: str
    entities: dict

@app.post("/extract", response_model=ExtractionResponse)
async def extract_entities(request: ExtractionRequest):
    """
    Extract medical entities from text
    """
    try:
        logger.info(f"Extracting entities from text (length: {len(request.text)})")
        entities = nlp_extractor.extract_entities(
            request.text, 
            request.layout_structure
        )
        
        return ExtractionResponse(
            status='success',
            entities=entities
        )
    
    except Exception as e:
        logger.error(f"Entity extraction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {'status': 'healthy', 'service': 'nlp-extraction'}
```

### FHIR Mapper Service API

```python
# src/api/fhir_api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from src.fhir_mapper.fhir_mapper import FHIRMapper
import logging

app = FastAPI(title="FHIR Mapping Service", version="1.0.0")

config = {'fhir_server_url': 'http://fhir-server:8080/fhir'}
fhir_mapper = FHIRMapper(config)

class MappingRequest(BaseModel):
    entities: dict
    document_type: str

@app.post("/map-to-fhir")
async def map_to_fhir(request: MappingRequest):
    """Convert entities to FHIR resources"""
    try:
        fhir_bundle = fhir_mapper.entities_to_fhir(
            request.entities,
            request.document_type
        )
        return {'status': 'success', 'fhir_bundle': fhir_bundle}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {'status': 'healthy', 'service': 'fhir-mapper'}
```

---

## ðŸ”„ Event-Driven Workflow

### Kafka Producer (Publishing Events)

```python
# src/event_middleware/kafka_middleware.py
from confluent_kafka import Producer
import json
import logging

logger = logging.getLogger(__name__)

class KafkaPublisher:
    def __init__(self, config):
        self.config = config
        self.producer = Producer({
            'bootstrap.servers': ','.join(config.get('bootstrap_servers', ['localhost:9092'])),
            'client.id': 'payerhub-producer'
        })
    
    def publish(self, topic, event_data):
        """
        Publish event to Kafka topic
        """
        try:
            # Serialize event data
            message = json.dumps(event_data).encode('utf-8')
            
            # Publish
            self.producer.produce(
                topic,
                value=message,
                callback=self._delivery_callback
            )
            
            # Wait for message delivery
            self.producer.flush()
            
            logger.info(f"Published event to topic: {topic}")
        
        except Exception as e:
            logger.error(f"Failed to publish event: {str(e)}")
            raise
    
    def _delivery_callback(self, err, msg):
        """Callback for message delivery"""
        if err:
            logger.error(f'Message delivery failed: {err}')
        else:
            logger.info(f'Message delivered to {msg.topic()} [{msg.partition()}]')


# Usage
publisher = KafkaPublisher({'bootstrap_servers': ['localhost:9092']})

event = {
    'event_type': 'prior_authorization.approved',
    'patient_id': 'patient-123',
    'auth_number': 'PA20250115A',
    'fhir_resource': {...}
}

publisher.publish('payer-hub.prior-auth.decisions', event)
```

### Kafka Consumer (Hub CRM Integration)

```python
# src/hub_integration/kafka_consumer.py
from confluent_kafka import Consumer, KafkaError
import json
import logging

logger = logging.getLogger(__name__)

class HubCRMConsumer:
    def __init__(self, config):
        self.config = config
        self.consumer = Consumer({
            'bootstrap.servers': ','.join(config.get('bootstrap_servers', ['localhost:9092'])),
            'group.id': config.get('group_id', 'hub-crm-consumer'),
            'auto.offset.reset': 'earliest'
        })
        
        # Subscribe to topics
        topics = [
            'payer-hub.eligibility.updates',
            'payer-hub.prior-auth.decisions',
            'payer-hub.claims.updates',
            'payer-hub.documents.processed'
        ]
        self.consumer.subscribe(topics)
    
    def consume_events(self, callback):
        """
        Consume events and process with callback function
        """
        try:
            while True:
                msg = self.consumer.poll(timeout=1.0)
                
                if msg is None:
                    continue
                
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        logger.error(f"Consumer error: {msg.error()}")
                        break
                
                # Parse event
                event_data = json.loads(msg.value().decode('utf-8'))
                
                # Process event with callback
                try:
                    callback(event_data)
                    logger.info(f"Processed event from {msg.topic()}")
                except Exception as e:
                    logger.error(f"Event processing failed: {str(e)}")
        
        finally:
            self.consumer.close()


# Usage Example: Hub CRM Integration
def process_hub_event(event):
    """Process event and update Hub CRM"""
    event_type = event['event_type']
    
    if event_type == 'prior_authorization.approved':
        # Update case in Hub CRM
        update_crm_case(
            patient_id=event['patient_id'],
            auth_number=event['auth_number'],
            status='approved'
        )
    
    elif event_type == 'eligibility.updated':
        # Update patient coverage in CRM
        update_patient_coverage(
            patient_id=event['patient_id'],
            coverage_data=event['fhir_resource']
        )

# Start consumer
consumer = HubCRMConsumer({'bootstrap_servers': ['localhost:9092']})
consumer.consume_events(process_hub_event)
```

---

## ðŸ“Š Monitoring & Observability

### Prometheus Metrics

```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# Define metrics
documents_processed = Counter(
    'documents_processed_total',
    'Total number of documents processed',
    ['document_type', 'status']
)

processing_time = Histogram(
    'document_processing_seconds',
    'Time spent processing documents',
    ['pipeline_stage']
)

anomalies_detected = Counter(
    'anomalies_detected_total',
    'Total number of anomalies detected',
    ['resource_type']
)

active_processing = Gauge(
    'documents_processing_active',
    'Number of documents currently being processed'
)

# Usage in pipeline
class MonitoredPipeline:
    def process_document(self, file_path, document_type):
        active_processing.inc()
        start_time = time.time()
        
        try:
            # Process document
            result = self.pipeline.process(file_path)
            
            # Record success
            documents_processed.labels(
                document_type=document_type,
                status='success'
            ).inc()
            
            return result
        
        except Exception as e:
            # Record failure
            documents_processed.labels(
                document_type=document_type,
                status='failure'
            ).inc()
            raise
        
        finally:
            # Record processing time
            duration = time.time() - start_time
            processing_time.labels(pipeline_stage='complete').observe(duration)
            active_processing.dec()

# Start metrics server
start_http_server(9090)  # Metrics available at http://localhost:9090/metrics
```

### Logging Configuration

```python
# src/logging_config.py
import logging
import sys
from logging.handlers import RotatingFileHandler
import json

class JSONFormatter(logging.Formatter):
    """Format logs as JSON for structured logging"""
    def format(self, record):
        log_data = {
            'timestamp': self.formatTime(record),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)

def setup_logging(log_level='INFO', log_file=None):
    """Configure logging for the application"""
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level))
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(JSONFormatter())
    logger.addHandler(console_handler)
    
    # File handler (if specified)
    if log_file:
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setFormatter(JSONFormatter())
        logger.addHandler(file_handler)
    
    return logger

# Usage
logger = setup_logging('INFO', '/app/logs/payerhub.log')
```

---

## ðŸ”’ Security Best Practices

### Environment Variables Management

```bash
# .env.example
# Copy to .env and fill in actual values

# Database
DB_PASSWORD=your_secure_password_here
DATABASE_URL=postgresql://user:${DB_PASSWORD}@postgres:5432/payerhub

# Security
JWT_SECRET_KEY=your_jwt_secret_key_here
ENCRYPTION_KEY=your_encryption_key_here

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_GROUP_ID=payer-hub-consumer

# Model Paths
MODEL_PATH=/app/models

# API Keys (for external services)
FHIR_SERVER_API_KEY=your_fhir_api_key
PAYER_API_KEY=your_payer_api_key

# Feature Flags
USE_GPU=true
ENABLE_ANOMALY_DETECTION=true
ENABLE_PRIVACY_LAYER=true
```

### API Authentication

```python
# src/auth/jwt_auth.py
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from datetime import datetime, timedelta
import os

security = HTTPBearer()

SECRET_KEY = os.getenv('JWT_SECRET_KEY')
ALGORITHM = "HS256"

def create_access_token(data: dict):
    """Create JWT access token"""
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(hours=24)
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    """Verify JWT token"""
    try:
        token = credentials.credentials
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    
    except JWTError:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication credentials"
        )

# Usage in API endpoint
@app.post("/process-document")
async def process_document(
    file: UploadFile,
    token_data: dict = Depends(verify_token)
):
    # Process document
    pass
```

---

## ðŸ§ª Testing Strategy

### Unit Tests

```python
# tests/test_ocr_processor.py
import pytest
from src.ai_pipeline.ocr_processor import OCRProcessor
from PIL import Image
import numpy as np

@pytest.fixture
def ocr_processor():
    config = {'use_layout_model': False}  # Faster for testing
    return OCRProcessor(config)

def test_process_simple_image(ocr_processor):
    """Test OCR on a simple generated image"""
    # Create test image with text
    img = Image.new('RGB', (800, 600), color='white')
    # ... add text to image
    
    result = ocr_processor._process_single_page(img, page_num=1)
    
    assert 'text' in result
    assert 'page_num' in result
    assert result['page_num'] == 1

def test_handles_invalid_file(ocr_processor):
    """Test error handling for invalid files"""
    with pytest.raises(Exception):
        ocr_processor.process_document('nonexistent_file.pdf')
```

### Integration Tests

```python
# tests/integration/test_pipeline.py
import pytest
from src.pipeline_orchestrator import PipelineOrchestrator

@pytest.fixture
def pipeline():
    config = {
        'ocr': {'use_layout_model': True},
        'nlp': {'use_biobert': True},
        'kafka': {'bootstrap_servers': ['localhost:9092']}
    }
    return PipelineOrchestrator(config)

def test_end_to_end_pipeline(pipeline):
    """Test complete pipeline with sample document"""
    result = pipeline.process_document_pipeline(
        file_path='tests/fixtures/sample_prior_auth.pdf',
        patient_id='test-patient-123',
        payer_id='test-payer',
        hub_id='test-hub'
    )
    
    assert result['status'] == 'completed'
    assert 'fhir_resources' in result
    assert len(result['steps_completed']) > 0
```

---

## ðŸ“ˆ Performance Optimization

### Caching Strategy

```python
# src/cache/redis_cache.py
import redis
import json
from functools import wraps

redis_client = redis.Redis(
    host='redis',
    port=6379,
    decode_responses=True
)

def cache_result(ttl=3600):
    """Decorator to cache function results in Redis"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key
            cache_key = f"{func.__name__}:{str(args)}:{str(kwargs)}"
            
            # Check cache
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # Execute function
            result = func(*args, **kwargs)
            
            # Store in cache
            redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result)
            )
            
            return result
        
        return wrapper
    return decorator

# Usage
@cache_result(ttl=1800)  # Cache for 30 minutes
def extract_entities_cached(text):
    return nlp_extractor.extract_entities(text)
```

### Batch Processing

```python
# src/batch/batch_processor.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

logger = logging.getLogger(__name__)

class BatchProcessor:
    def __init__(self, orchestrator, max_workers=5):
        self.orchestrator = orchestrator
        self.max_workers = max_workers
    
    def process_batch(self, file_paths):
        """Process multiple documents in parallel"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            future_to_file = {
                executor.submit(
                    self.orchestrator.process_document_pipeline,
                    file_path,
                    f'patient-{i}',
                    'payer-1',
                    'hub-1'
                ): file_path
                for i, file_path in enumerate(file_paths)
            }
            
            # Collect results
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    results.append({
                        'file': file_path,
                        'status': result['status']
                    })
                    logger.info(f"Processed: {file_path}")
                except Exception as e:
                    logger.error(f"Failed to process {file_path}: {str(e)}")
                    results.append({
                        'file': file_path,
                        'status': 'failed',
                        'error': str(e)
                    })
        
        return results

# Usage
batch_processor = BatchProcessor(orchestrator, max_workers=10)
results = batch_processor.process_batch([
    'doc1.pdf',
    'doc2.pdf',
    'doc3.pdf'
])
```

---

## ðŸš€ Deployment Checklist

### Pre-Production

- [ ] All unit tests passing
- [ ] Integration tests passing
- [ ] Load testing completed
- [ ] Security audit completed
- [ ] HIPAA compliance review
- [ ] Models trained and validated
- [ ] Monitoring dashboards configured
- [ ] Alerting rules set up
- [ ] Backup strategy in place
- [ ] Disaster recovery plan documented

### Production Launch

- [ ] Deploy to staging environment
- [ ] Run smoke tests
- [ ] Deploy to production
- [ ] Monitor logs and metrics
- [ ] Verify Kafka event flow
- [ ] Check FHIR resource creation
- [ ] Validate privacy controls
- [ ] Test failover scenarios

### Post-Production

- [ ] Monitor performance metrics
- [ ] Track error rates
- [ ] Review anomaly detection accuracy
- [ ] Collect user feedback
- [ ] Plan model retraining schedule
- [ ] Document lessons learned

---

## ðŸ“Š Key Performance Indicators (KPIs)

### System Health
- Uptime: Target 99.9%
- API Response Time: < 500ms (p95)
- Document Processing Time: < 5 minutes
- Event Delivery Latency: < 1 second

### Accuracy Metrics
- OCR Accuracy: > 95%
- NER F1 Score: > 0.90
- FHIR Mapping Completeness: > 90%
- Anomaly Detection Precision: > 0.85

### Business Metrics
- Automation Rate: > 70%
- Cost per Document: < $0.50
- Manual Review Rate: < 20%
- Time Savings: > 50%

---

## ðŸŽ“ Training Documentation

### For Developers

1. **Setup Development Environment**: Follow README.md
2. **Understand Architecture**: Review architecture diagrams
3. **Run Local Tests**: Execute test suite
4. **Make Changes**: Follow coding standards
5. **Deploy**: Use CI/CD pipeline

### For Operators

1. **Monitor Dashboards**: Check Grafana/Prometheus
2. **Review Logs**: Use ELK stack or CloudWatch
3. **Handle Alerts**: Follow runbook
4. **Scale Services**: Adjust replica counts
5. **Update Models**: Follow model deployment process

### For Data Scientists

1. **Access Training Data**: Query database
2. **Train Models**: Use training scripts
3. **Evaluate Performance**: Run evaluation metrics
4. **Deploy Models**: Follow model versioning
5. **Monitor Model Drift**: Track prediction quality

---

**Document Version**: 1.0  
**Last Updated**: October 26, 2025  
**Maintained By**: PayerHub Platform Team
